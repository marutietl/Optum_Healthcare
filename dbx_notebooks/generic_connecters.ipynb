{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4885534d-1e82-42d1-842c-ac610373d1bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def spark_connector():\n",
    "    spark.conf.set(\"fs.azure.account.key.saadlsoptum.dfs.core.windows.net\",dbutils.secrets.get(scope='optumscope',key='adlskey'))\n",
    "    return \"connected to adls\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2db4ad1-aa2b-443f-abbc-67cdcff5daf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_bronze_csv_data(fn):\n",
    "    df=spark.read.csv(\"abfss://optum@saadlsoptum.dfs.core.windows.net/Bronze/\"+fn+\".csv\",header=True,inferSchema=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ff355b-755a-498e-a10b-31ff306eed26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Generic code to read file from silver layer\n",
    "def read_silver_csv_data(fn):\n",
    "    df=spark.read.csv(\"abfss://optum@saadlsoptum.dfs.core.windows.net/Silver/\"+fn+\".csv\",header=True,inferSchema=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd017e21-9309-4f38-8691-aaf6eb8a320e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_bronze_json_data(fn):\n",
    "    df=spark.read.json(\"abfss://optum@saadlsoptum.dfs.core.windows.net/Bronze/\"+fn+\".json\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c087347a-99a4-4b1e-aba1-ca277e951fac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime, uuid\n",
    "\n",
    "def write2silver(df, file_name):\n",
    "    silver_path = \"abfss://optum@saadlsoptum.dfs.core.windows.net/Silver/\"\n",
    "    \n",
    "    # unique temp folder for this run\n",
    "    unique_id = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + \"_\" + str(uuid.uuid4())\n",
    "    temp_path = f\"{silver_path}/output_temp/{unique_id}/\"\n",
    "    \n",
    "    # always a single CSV file at final path\n",
    "    final_file = f\"{silver_path}/{file_name}.csv\"\n",
    "    \n",
    "    # write to temp\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(temp_path)\n",
    "    \n",
    "    # pick actual CSV file from Spark output\n",
    "    files = dbutils.fs.ls(temp_path)\n",
    "    csv_files = [f.path for f in files if f.name.endswith(\".csv\")]\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise Exception(\"No CSV file found in temp path\")\n",
    "    \n",
    "    csv_file = csv_files[0]\n",
    "    \n",
    "    # move & overwrite final\n",
    "    dbutils.fs.mv(csv_file, final_file, True)\n",
    "    \n",
    "    # cleanup temp folder\n",
    "    dbutils.fs.rm(temp_path, recurse=True)\n",
    "    \n",
    "    print(f\"✅ File successfully written to Silver: {final_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee1af8b-b3d9-475f-b64c-24ee9bed7c08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime, uuid\n",
    "\n",
    "def write2gold(df, file_name):\n",
    "    gold_path = \"abfss://optum@saadlsoptum.dfs.core.windows.net/Gold/\"\n",
    "    \n",
    "    # Unique temp folder for staging\n",
    "    unique_id = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + \"_\" + str(uuid.uuid4())\n",
    "    temp_path = f\"{gold_path}/output_temp/{unique_id}/\"\n",
    "    \n",
    "    # Final single CSV file\n",
    "    final_file = f\"{gold_path}/{file_name}.csv\"\n",
    "    \n",
    "    # Write DataFrame to temp path\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(temp_path)\n",
    "    \n",
    "    # Pick Spark’s actual CSV file\n",
    "    files = dbutils.fs.ls(temp_path)\n",
    "    csv_file = [f.path for f in files if f.name.endswith(\".csv\")][0]\n",
    "    \n",
    "    # Move & overwrite final file\n",
    "    dbutils.fs.mv(csv_file, final_file, True)\n",
    "    \n",
    "    # Cleanup temp folder\n",
    "    dbutils.fs.rm(temp_path, recurse=True)\n",
    "    \n",
    "    print(f\"✅ File successfully written to Gold: {final_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "generic_connecters",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
